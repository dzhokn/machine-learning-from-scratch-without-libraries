{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic Regression is like a linear regression but used for another type of problems. Linear regression predicts a **continuous** outcome, while logistic regression predicts a **categorical** (often binary) outcome. Therefore, logistic regressions are used for **classification** problems. \n",
    "\n",
    "In essence:\n",
    "* If you need to predict a number, use linear regression.\n",
    "* If you need to predict a category or probability of an event, use logistic regression.\n",
    "\n",
    "### Why is it called regression if it is a classification algorithm?\n",
    "The term “regression” in logistic regression refers to the fact that the model is trying to ****predict the probability** of a categorical outcome, such as whether a basketball player will win in 1x1 match.\n",
    "\n",
    "In logistic regression, the probability of the outcome is represented by a logistic function. The logistic function is a sigmoid curve that has a range of values from 0 to 1. This means that the predicted probability of the outcome can be any value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### 1. Calculate Y\n",
    "In logistic regression, we aim to model the probability of a binary outcome (e.g., $0$ or $1$) based on input features. The hypothesis function of logistic regression is defined as follows:\n",
    "$$ p(y = 1) = \\frac{1}{(1 + \\exp(-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2)))} $$\n",
    "\n",
    "where:\n",
    "* $p(y = 1)$ is the probability of the binary outcome being equal to $1$\n",
    "* $\\beta_0$ is the intercept\n",
    "* $\\beta_1$ and $\\beta_2$ are the coefficients for the predictor variables $x_1$ and $x_2$\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"img/sigmoid-curve.png\" alt=\"Sigmoid curve\" width=\"500\" height=\"300\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 1.</b> Sigmoid curve (between 0 and 1)</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_y(x: np.ndarray, w: np.ndarray, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "      z (float)     : A scalar\n",
    "\n",
    "    Returns:\n",
    "      y (float)     : sigmoid(z)\n",
    "    \"\"\"\n",
    "    z = __compute_z(x, w, b)\n",
    "    return __sigmoid(z)\n",
    "\n",
    "def __sigmoid(z: float) -> float:\n",
    "    return 1 / (1 + np.exp(-z)) # Sigmoid function which returns a value between 0 and 1\n",
    "\n",
    "def __compute_z(x: np.ndarray, w: np.ndarray, b: float) -> float:\n",
    "    return b + w @ x # (1, n) * (n, 1) = (1,) , where n is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute loss\n",
    "#### Why MSE is Not Suitable for Logistic Regression\n",
    "In classification tasks, where the output is a probability between $0$ and $1$ we use the **sigmoid function** to predict these probabilities.\n",
    "\n",
    "If we try to use the **Mean Squared Error** for Logistic Regression, we would face several problems:\n",
    "* **Nonlinearity of the Sigmoid Function**: The sigmoid function introduces nonlinearity, and when we plug it into the MSE formula, the cost function becomes non-convex. This means it could have **multiple local minima**, making it harder for Gradient Descent to find the optimal solution.\n",
    "* **Squaring Errors**: The MSE squares the diff between the predicted probability and the actual class label ($0$ or $1$). When the prediction is far from the actual value, the error gets magnified. However, because the outputs of Logistic Regression are probabilities (values between $0$ and $1$), squaring these small diffs can make it difficult for the model to learn effectively.\n",
    "\n",
    "#### The Solution: Log Loss (Cross-Entropy)\n",
    "Instead of MSE, Logistic Regression uses a different cost function called Log Loss (or Cross-Entropy). Log Loss penalizes incorrect predictions more effectively than MSE and helps the model learn to improve. It works by taking the **logarithm of predicted probabilities**, allowing the model to focus on making confident predictions closer to the true class labels.\n",
    "\n",
    "The loss is the diff between predicted and actual $y$ values.\n",
    "$$ J = - \\sum y log(\\hat{y}) + (1-y) log(1-\\hat{y}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X: np.ndarray, Y: np.ndarray, w: np.ndarray, b: float) -> float:\n",
    "  \"\"\"Calculates the loss (cost) for a given X (set of vectors), Y (target values), w (weights), and b (bias).\"\"\" \n",
    "  m = X.shape[0]        # The number of examples (rows) in the training set\n",
    "  loss = 0.\n",
    "\n",
    "  for i in range(m):\n",
    "    y_pred = compute_y(X[i], w, b)\n",
    "    loss += -(Y[i] * np.log(y_pred) + (1-Y[i]) * np.log(1-y_pred)) # Cross-Entropy\n",
    "  return loss / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient descent\n",
    "We'll do a gradient descent in order to optimize the weights.\n",
    "\n",
    "\n",
    "$$ \\frac{dJ}{dw} = X^T (\\hat{y} - y) $$\n",
    "\n",
    "$$ \\frac{dJ}{db} = \\sum (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X: np.ndarray, Y: np.ndarray, w: np.ndarray, b: float) -> tuple[np.ndarray, float]: \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression with multiple features.\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))       : input data, m rows with n features\n",
    "      Y (ndarray (m,))        : target values\n",
    "      w (ndarray (n,))        : model weights  \n",
    "      b (scalar)              : model bias\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,))    : The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)          : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    # Calculate all the predicted values\n",
    "    m = X.shape[0]               # number of rows\n",
    "    Y_pred = np.zeros(m)\n",
    "    for i in range(m):\n",
    "      Y_pred[i] = compute_y(X[i], w, b)\n",
    "    \n",
    "    dj_dw = X.T @ (Y_pred - Y) / m\n",
    "    dj_db = np.sum(Y_pred - Y) / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train\n",
    "\n",
    "After each iteration of the gradient descent we need to update the weights and store the current loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X: np.ndarray, Y: np.ndarray, weights: np.ndarray, bias: float, learn_rate: float, epochs: int): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with specified learning rate.\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))       : input data, m examples with n features\n",
    "      Y (ndarray (m,))        : target values\n",
    "      weights (ndarray (n,))  : initial model parameters  \n",
    "      bias (float)            : initial model parameter\n",
    "      learn_rate (float)      : Learning rate\n",
    "      epochs (int)            : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      weights (ndarray (n,))  : Updated values of parameters \n",
    "      bias (float)            : Updated value of parameter \n",
    "      losses (list)           : History of losses\n",
    "      \"\"\"\n",
    "    losses = []            # An array to store history of losses\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Calculate the gradient for these `w` and `b`\n",
    "        dj_db, dj_dw = compute_gradient(X, Y, weights, bias)\n",
    "\n",
    "        # Update `w` and `b` based on the gradient (up or down) and the specified learning rate\n",
    "        weights -= learn_rate * dj_dw   # vector operation (w and dj_dw are vectors with the same size)\n",
    "        bias -= learn_rate * dj_db\n",
    "\n",
    "        # Save loss at each iteration\n",
    "        losses.append( compute_loss(X, Y, weights, bias) )\n",
    "\n",
    "        # Print the cost after every 10% of the iterations\n",
    "        if _ % 1000 == 0:\n",
    "            print(f\"Iteration {_} - Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "    return weights, bias, losses # return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict\n",
    "We need a method that makes binary predictions. To convert the predicted probabilities into binary predictions, a threshold ($0.5$) is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: np.ndarray, w: np.ndarray, b: float) -> float:\n",
    "    '''\n",
    "    Predict the probability of a binary outcome based on the input features.\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): A vector (observation)\n",
    "\n",
    "    Returns:\n",
    "      y_pred (int): 1 if sigmoid(z) > 0.5 else 0\n",
    "    '''\n",
    "    y_pred = compute_y(x, w, b)\n",
    "    return 1 if y_pred > 0.5 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate model accuracy\n",
    "Compare the predictions of a model to the actual outcome values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X: np.ndarray, Y: np.ndarray, w: np.ndarray, b: float) -> float:\n",
    "    '''\n",
    "    Compute the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "      X (ndarray): A dataset of input features\n",
    "      Y (ndarray): A dataset of target values\n",
    "      w (ndarray): A vector of weights\n",
    "      b (float): A scalar\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    correct_predictions = 0\n",
    "    for i in range(m):\n",
    "        y_pred = predict(X[i], w, b)\n",
    "        if y_pred == Y[i]:\n",
    "            correct_predictions += 1\n",
    "    return correct_predictions / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Demo\n",
    "Let's execute a demo. Again, by using the sample data for monthly salaries in the ML industry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss: 0.6722\n",
      "Iteration 1000 - Loss: 0.5479\n",
      "Iteration 2000 - Loss: 0.5115\n",
      "Iteration 3000 - Loss: 0.4944\n",
      "Iteration 4000 - Loss: 0.4850\n",
      "Iteration 5000 - Loss: 0.4794\n",
      "Iteration 6000 - Loss: 0.4758\n",
      "Iteration 7000 - Loss: 0.4734\n",
      "Iteration 8000 - Loss: 0.4717\n",
      "Iteration 9000 - Loss: 0.4706\n",
      "Iteration 10000 - Loss: 0.4697\n",
      "Iteration 11000 - Loss: 0.4691\n",
      "Iteration 12000 - Loss: 0.4686\n",
      "Iteration 13000 - Loss: 0.4683\n",
      "Iteration 14000 - Loss: 0.4680\n",
      "Iteration 15000 - Loss: 0.4678\n",
      "Iteration 16000 - Loss: 0.4677\n",
      "Iteration 17000 - Loss: 0.4675\n",
      "Iteration 18000 - Loss: 0.4674\n",
      "Iteration 19000 - Loss: 0.4674\n",
      "\n",
      "Final weights: [ 0.51722429 -0.00917398] and bias: -0.03557769704347439\n",
      "Accuracy: 67%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X1 = Basketball players - years of experience\n",
    "X1 = [1.2, 1.3, 1.5, 1.8, 2, 2.1, 2.2, 2.5, 2.8, 2.9, 3.1, 3.3, 3.5, 3.8, 4, 4.1, 4.5, 4.9, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 10, 11, 12, 13, 14, 15]\n",
    "# X2 = Height (cm)\n",
    "X2 = [181, 190, 202, 185, 178, 189, 204, 192, 183, 184, 195, 196, 197, 188, 189, 190, 191, 182, 183, 190, 185, 186, 197, 191, 189, 200, 186, 189, 191, 190, 195, 193, 187, 192, 190, 187]\n",
    "# Y = Win or Lose\n",
    "Y = [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Merge the X1 and X2 into a single X\n",
    "X = [[x1, x2] for x1, x2 in zip(X1, X2)]\n",
    "\n",
    "# Convert the X, Y to a numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Configure gradient descent settings\n",
    "learn_rate = 0.0002   # learning rate (step size)\n",
    "epochs = 20000   # number of iterations (epochs)\n",
    "weights = np.zeros(X.shape[1])\n",
    "bias = 0\n",
    "\n",
    "weights, bias, losses = fit(X, Y, weights, bias, learn_rate, epochs)\n",
    "\n",
    "print(f\"\\nFinal weights: {weights} and bias: {bias}\")\n",
    "print(f\"Accuracy: {compute_accuracy(X, Y, weights, bias) * 100:.0f}%\")\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(np.arange(epochs), losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* (Understanding Logistic Regression: A Beginner’s Guide)[https://medium.com/@novus_afk/understanding-logistic-regression-a-beginners-guide-73f148866910]\n",
    "* (Logistic Regression)[https://medium.com/@RobuRishabh/logistic-regression-c2d2bac7afd8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
