{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Multilayer Neural Network work?\n",
    "Now let’s understand how a Neural Network is represented. A neural network consists of many **Nodes** (Neurons) grouped in **Layers**. Each layer can have any number of nodes and a neural network can have any number of layers. Let’s have a closer look at a couple of layers.\n",
    "\n",
    "<center><img src=\"img/neural_network_1.png\" alt=\"Neural Network Basic Layers\" width=\"346\" height=\"419\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 1.</b> Layers in a Neural Network</i></p>\n",
    "\n",
    "Now as you can see, there are many interconnections between both the layers. These interconnections exist between **each node** in the first layer with **each and every node** in the second layer. These are also called the **weights** between two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "Now let’s see how exactly these **weights** operate.\n",
    "<center><img src=\"img/neural_network_2.png\" alt=\"Neural Network Basic Layers\" width=\"500\" height=\"393\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 2.</b> How single neuron operates</i></p>\n",
    "\n",
    "$$ H = x_1*w_1 + x_2*w_2 + x_3*w_3 + b $$\n",
    "$$ Y = f(H) $$\n",
    "\n",
    "Here we take the example of what’s going on with a **single node** in the network. Here we are considering all the values from the **previous layer** connecting to **one node in the next layer**.\n",
    "\n",
    "* $Y$ is the **final output value** of the node.\n",
    "* $w_i$ are the **weights** between the nodes in the previous layer and the output node.\n",
    "* $x_i$ are the **values of the nodes** of the previous layer.\n",
    "* $b$ is a **constant** bias. Bias is essentially a weight without an input term. It’s useful for having an **extra bit of adjustability** which is not dependant on previous layer.\n",
    "* $H$ is the *intermediate node value*. This is not the final value of the node.\n",
    "* $f( )$ is called an **Activation Function** and it is something we can choose. We will go through its importance later.\n",
    "\n",
    "So finally, the output value of this node will be $f(0.57)$\n",
    "\n",
    "Now let’s look at the calculations between two complete layers:\n",
    "\n",
    "<center><img src=\"img/neural_network_3.png\" alt=\"Calculations between Neural Network layers\" width=\"500\" height=\"583\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 3.</b> Calculations between two layers</i></p>\n",
    "\n",
    "The weights in this case have been colour coded for easier understanding. We can represent the entire calculation as a matrix multiplication. If we represent the weights corresponding to each input node as vectors and arrange them horizontally, we can form a matrix, this is called the weight matrix. Now we can multiply the **weight matrix** with the input vector and then add the bias vector to get the intermediate node values.\n",
    "\n",
    "<center><img src=\"img/neural_network_4.png\" alt=\"Formula for calculating output values of a layer\" width=\"500\" height=\"481\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 4.</b> Formula for calculating output values of a layer</i></p>\n",
    "\n",
    "We can summarize the entire calculation as $Y = f(w*x + b)$. Here, $Y$ is the output vector, $x$ is the input vector, $w$ represents the weight matrix between the two layers and $b$ is the bias vector.\n",
    "\n",
    "We can determine the size of the weight matrix by looking at the number of input nodes and output nodes. An $m*n$ weight matrix means that it is between two layers with the first layer having $n$ nodes and the second layer having $m$ nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "Now let’s look at a complete neural network.\n",
    "\n",
    "\n",
    "<center><img src=\"img/neural_network_5.png\" alt=\"Formula for calculating the whole neural network\" width=\"500\" height=\"542\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 5.</b> Formula for calculating the whole neural network</i></p>\n",
    "\n",
    "This is a small neural network of four layers. The input layer is where we feed our **external stimulus**, or basically the **data** from which our neural network has to **learn from**. The output layer is where we are supposed to get the target value, this represents what exactly our neural network is trying to **predict** or **learn**. All layers in between are called **hidden layers**. When we feed the inputs into the first layer, the values of the nodes will be calculated layer by layer using the matrix multiplications and activation functions till we get the final values at the output layer. That is how we get an output from a neural network.\n",
    "\n",
    "So essentially a neural network is, simply put, a series of matrix multiplications and activation functions. When we input a vector containing the input data, the data is multiplied with the sequence of weight matrices and subjected to activation functions untill it reaches the output layer, which contains the **predictions** of the neural network corresponding to that particular input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now let's implement a basic multilayer perceptron (neural network). For the sake of the example we will be generating **random weights** for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        # Initialize weights and biases\n",
    "        # NB: Remember, the weight matrix size is m x n, where m is the number of output nodes and n is the number of input nodes\n",
    "        weight_matrix_1 = np.random.randn(hidden_size, input_size) # Weight matrix between input and hidden layer\n",
    "        weight_matrix_2 = np.random.randn(output_size, hidden_size) # Weight matrix between hidden and output layer\n",
    "\n",
    "        self.weights = [weight_matrix_1, weight_matrix_2]\n",
    "        self.biases = [np.zeros(hidden_size), np.zeros(output_size)]\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray: # Activation function (there are many other activation functions)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> list[np.ndarray]:\n",
    "        activations = [x]           # List to store activations for each layer. Keep the first input X too.\n",
    "        current_activation = x      # Current activation value (the first time it is simply the input X)\n",
    "        # Propagate through each layer\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, current_activation) + b   # Apply the layer's weights and bias\n",
    "            current_activation = self.sigmoid(z)    # Apply activation function and get the layer's output\n",
    "            activations.append(current_activation)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invoke the neural network (remember, we use random weights) and see how our input data of three numbers $(1,2,3)$ is converted into a final output after passing through all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51708171 0.1239638 ]\n"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork(3, 4, 2)  # 3 input nodes, 4 hidden nodes, 2 output nodes\n",
    "input_data = np.array([1, 2, 3])\n",
    "final_layer = network.forward(input_data)[-1]  # Get final layer output\n",
    "\n",
    "print(final_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of activation function\n",
    "Even though our neural network has a very complex configuration of weights, it will not be able to solve a problem without the activation function. The reason for this lies in the concept of **Non-Linearity**.\n",
    "\n",
    "Let’s revise what linearity and non-linearity means.\n",
    "\n",
    "$$ y = w_1 * x_1 + w_2 * x_2 $$\n",
    "\n",
    "The above equation represents a **linear relationship** between $y$ and $x_1$, $x_2$. Regardless of what values $w_1$ and $w_2$ have, at the end of the day the change of value of $x_1$ and $x_2$ will result in a linear change in $y$. Now if we look at real world data we realize this is actually not desirable because data often has non-linear relationships between the input and output variables.\n",
    "\n",
    "\n",
    "<center><img src=\"img/activation_function_1.png\" alt=\"Linearity vs non-linearity\" width=\"500\" height=\"409\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 6.</b> Linear relationship vs non-linear relationship</i></p>\n",
    "\n",
    "The above diagram represents a typical dataset which shows a non-linear relationship between $x$ and $y$. If we try to fit a linear relationship on the data, we will end up with the **red line**, which is not a very accurate representation of the data. However if our relationship can be **non-linear**, we are able to get the green line, which is much better.\n",
    "\n",
    "Now let’s compare the neural network equation **with and without the activation function**.\n",
    "\n",
    "Without activation:\n",
    "$$y = \\sum_{i=0}^{n}(w_i * x_i) + b $$\n",
    "\n",
    "With activation:\n",
    "$$ y = f(\\sum_{i=0}^{n}(w_i * x_i) + b) $$\n",
    "\n",
    "We can observe that in this equation, there exists a **linear relationship** between the input and the output. However in the case of the equation with activation function, we can say that the relationship between input and output can be non-linear, IF the activation function itself is **non-linear**.\n",
    "\n",
    "Hence all we have to do is keep some non-linear function as the activation function for each neuron and our neural network is now capable of fitting on non-linear data.\n",
    "\n",
    "Let’s look at a couple of popular activation functions:\n",
    "\n",
    "\n",
    "<center><img src=\"img/activation_function_2.png\" alt=\"ReLU vs Sigmoid\" width=\"500\" height=\"283\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 7.</b> ReLU vs Sigmoid activation functions  </i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "ReLU stands for **Rectified Linear Unit**. It essentially becomes an identity function ($y = x$) when $x \\geq 0$ and becomes $0$ when $x \\lt 0$. This is a very widely used activation function because its a non-linear function and it is very simple.\n",
    "\n",
    "### Sigmoid\n",
    "Sigmoid is essentially a function bounded between $0$ and $1$. It will become $0$ for values which are very negative and $1$ for values which are very positive. Hence this function squishes values which are very high or very low to values between $0$ and $1$. This is useful in neural networks sometimes to ensure values aren’t extremely high or low. This function is usually used at the last layer when we need values which are binary ($0$ or $1$).\n",
    "\n",
    "$$y=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "### Tanh\n",
    "Tanh function (hyperbolic tangent function), is a shifted version of the sigmoid, allowing it to stretch across the y-axis. It is defined as:\n",
    "$$y=tanh(x) = \\frac{2}{1+e^{-2x}}-1$$\n",
    "\n",
    "Alternatively, it can be expressed using the sigmoid function:\n",
    "$$y=tanh(x) = 2 \\times sigmoid(2x) -1$$\n",
    "\n",
    "**Tanh** outputs values from $-1$ to $+1$. It is commonly used in hidden layers due to its zero-centered output, facilitating easier learning for subsequent layers.\n",
    "\n",
    "### Softmax\n",
    "Softmax is an activation function commonly used in neural networks for multi-classification problems. Softmax is ideal for problems involving **more than two classes**, where the goal is to predict a single class out of many. The function's ability to generate a **probability distribution** over classes makes it particularly useful in classification models.\n",
    "\n",
    "$$y=softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linearity importance in NN\n",
    "\n",
    "Neural networks consist of neurons that operate using **weights**, **biases**, and **activation functions**.\n",
    "\n",
    "In the learning process, these weights and biases are updated based on the error produced at the output - a process known as **backpropagation**. Activation functions enable backpropagation by providing gradients that are essential for updating the weights and biases.\n",
    "\n",
    "Without non-linearity, even deep networks would be limited to solving **only simple, linearly separable problems**. Activation functions empower neural networks to model highly complex data distributions and solve advanced deep learning tasks. Adding non-linear activation functions introduce flexibility and enable the network to learn more complex and abstract patterns from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training \n",
    "\n",
    "Now that we have seen how a neural network is represented, we can go on to see how exactly it works. Since there are many layers having many neurons, there exists a complex set of weights to get an output from some input variables. Each weight in this network can be changed and hence there are countless configurations a neural network can have.\n",
    "\n",
    "A **trained** neural network has weights configuration which accurately predicts correct outputs for some input data. And that is what we ultimately hope to achieve. We will now go through how exactly a neural network trains itself to get this desirable weight configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Backpropagation is the name of the algorithm a neural network uses to train itself. This revolutionary algorithm is a mixture of the **chain rule in derivation** and **gradient descent**, the common optimization algorithm used in linear and logistic regression.\n",
    "\n",
    "To understand how backpropagation works, first we have to understand the relationship between the output and the weights in between. It is clear that every weight in the neural network will affect the output in some way due to the way the neural network is connected. Due to this fact, we can say that if I **change** a particular weight, the output will **change** in some way. We can also find the exact mathematical equation defining the relationship between each weight and the output.\n",
    "\n",
    "<center><img src=\"img/backprop_1.png\" alt=\"Dataset with three input features\" width=\"500\" height=\"233\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 8.</b> A dataset with three input features (X1, X2, and X3) </i></p>\n",
    "\n",
    "Let’s say we are given a dataset with three input features, ($x_1$, $x_2$ and $x_3$) and we need to find the relationship between the input features and the output using a neural network. We will now see what exactly the neural network does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimize the error\n",
    "\n",
    "<center><img src=\"img/backprop_2.png\" alt=\"error between the output and expected value\" width=\"580\" height=\"301\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 9.</b> Feed the neural network and measure the error between the output and expected value</i></p>\n",
    "\n",
    "First step is to feed in the data and getting the output from the neural network. We shall call the output, `Y_pred`. Next we should compare the predicted value with the actual value. This value will be called the error. It is essentially how bad or how far off the model is from predicting the values correctly. Our goal is to now **minimize the error**.\n",
    "\n",
    "Since `Y_pred` is a function of all the weights in the model and `Error` is a function of `Y_pred`, we can say that the `Error` will also depend on the weights. This means that we need to adjust our weights in such a way that the error is minimized. We do that using **partial derivatives**.\n",
    "\n",
    "Let’s take a simple example of an equation, $y = w_1*x_1 + w_2*x_2$. If we find the partial derivative of $w_1$ or $w_2$ with respect to $y$, we can find out how $w_1$ or $w_2$ can affect $y$.\n",
    "\n",
    "If the partial derivative of $w_1$ with respect to $y$ is **POSITIVE**, that means **DECREASING** the weight will **DECREASE** $y$.\n",
    "\n",
    "If the partial derivative of $w_1$ with respect to $y$ is **NEGATIVE**, that means **INCREASING** the weight will **DECREASE** $y$.\n",
    "\n",
    "\n",
    "<center><img src=\"img/backprop_3.png\" alt=\"Derivatives measure the slope\" width=\"500\" height=\"294\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 10.</b> Derivatives measure the slope and in which direction to descent in order to change the output value</i></p>\n",
    "\n",
    "These two rules are all we need to optimize the weights in the neural network. We need to find the **partial derivative of the error** with respect to every weight. If that partial derivative is **positive**, then we **decrease** the value of that weight so that error gets decreased. If that partial derivative is **negative**, then we **increase** that weight so that the error gets decreased. This is the basic underlying concept of how weights are updated after we calculate the error.\n",
    "\n",
    "Since the last layer is the closest to the error, we will first derive the last layer with respect to the error and update those weights. Then we will move to the second last layer to do the same and so on and so forth. We repeat this process till we reach the first layer and all the weights are updated. This entire process is called **backpropagation**.\n",
    "\n",
    "<center><img src=\"img/backprop_4.gif\" alt=\"Backpropagation\" width=\"640\" height=\"480\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 11.</b> Backpropagation - Derive and update</i></p>\n",
    "\n",
    "We perform backpropagation for a single row of data and update the weights. We then repeat for all the data available in the training data set, this entire cycle is called **one epoch**. Usually neural networks can take several epochs to train and it is up to us to decide how many epochs it will train for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical explanation\n",
    "\n",
    "Let's say we have a neural network with 4 layers, each of them 1 neuron.\n",
    "\n",
    "Let's name the output of each layer $a^{(L)}$, $a^{(L-1)}$, $a^{(L-2)}$, $a^{(L-3)}$\n",
    "\n",
    "<center><img src=\"img/backprop_10.png\" alt=\"Backpropagation Simple NN\" width=\"500\" height=\"154\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 12.</b> Simple network with 4 neurons</i></p>\n",
    "\n",
    "The desired output $y$ is $1$.\n",
    "\n",
    "So the cost of this simple network is:\n",
    "$$ C_0 = (a^{(L)} - y)^2$$\n",
    "\n",
    "where $C_0$ is the cost for single training example.\n",
    "\n",
    "<center><img src=\"img/backprop_11.png\" alt=\"Backpropagation Cost calculation\" width=\"500\" height=\"251\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 13.</b> Cost calculation</i></p>\n",
    "\n",
    "The last ouput $a^{(L)}$ is calculated by next formula:\n",
    "$$a^{(L)} = \\sigma(w^{(L)} a^{(L-1)} + b^{(L)}) $$\n",
    "\n",
    "where $\\sigma$ is the activation function (e.g. *sigmoid* or *ReLU*)\n",
    "\n",
    "For the sake of clarity let's name the input for the activation function $z$ :\n",
    "$$z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)} $$\n",
    "\n",
    "$$a^{(L)} = \\sigma(z^{(L)}) $$\n",
    "\n",
    "So, now we need to identify the importance of each weight $w$ to the cost. Because by modifying the weights we can modify (minimize) the cost. To do this we need to calculate the derivative of $w^{(L)}$ w.r.t $C_0$.\n",
    "\n",
    "$$ \\frac{dC_0}{dw^{(L)}}$$\n",
    "\n",
    "Using the **Chain Rule** we directly come up with:\n",
    "\n",
    "\n",
    "$$ \\frac{dC_0}{dw^{(L)}} = \\frac{dz^{(L)}}{dw^{(L)}} \\frac{da^{(L)}}{dz^{(L)}} \\frac{dC_0}{da^{(L)}}$$\n",
    "\n",
    "This would give us the sensitivity of $C$ to small changes in $w^{(L)}$.\n",
    "\n",
    "Now, let's calculate all the three derivatives, starting from the last one:\n",
    "\n",
    "$$ \\frac{dC_0}{da^{(L)}} = \\frac{d(a^{(L)} - y)^2}{da^{(L)}} = 2(a^{(L)} - y) $$\n",
    "\n",
    "The derivative of $a^{(L)}$ w.r.t $z^{(L)}$ is simply the derivative of the activation function:\n",
    "\n",
    "$$ \\frac{da^{(L)}}{dz^{(L)}} =  \\frac{d\\sigma(z^{(L)})}{dz^{(L)}} =   \\sigma ' (z^{(L)}) $$\n",
    "\n",
    "And the derivative of $z^{(L)}$ w.r.t $w^{(L)}$ comes up to be:\n",
    "\n",
    "$$ \\frac{dz^{(L)}}{dw^{(L)}} = \\frac{d(w^{(L)} a^{(L-1)} + b^{(L)})}{dw^{(L)}} = a^{(L-1)} $$\n",
    "\n",
    "So, finally the derivative of $C_0$ w.r.t to $w^{(L)}$ is:\n",
    "\n",
    "$$  \\boxed{\\frac{dC_0}{dw^{(L)}} = a^{(L-1)} \\sigma '(z^{(L)}) 2(a^{(L)} - y)}  $$\n",
    "\n",
    "Since we have not a single, but many training examples, the derivative of the full cost function for all examples could simply be calculated as the average of all training examples:\n",
    "\n",
    "$$ \\frac{dC}{dw^{(L)}} = \\frac{1}{m} \\sum_{i=0}^{m-1}\\frac{dC_i}{dw^{(L)}}$$\n",
    "\n",
    "The sensitivity to the bias is almost identical:\n",
    "\n",
    "\n",
    "$$ \\frac{dC_0}{db^{(L)}} = \\frac{dz^{(L)}}{db^{(L)}} \\frac{da^{(L)}}{dz^{(L)}} \\frac{dC_0}{da^{(L)}}$$\n",
    "\n",
    "and evaluated to:\n",
    "\n",
    "\n",
    "$$  \\boxed{\\frac{dC_0}{db^{(L)}} = 1 \\sigma '(z^{(L)}) 2(a^{(L)} - y)}  $$\n",
    "\n",
    "Now, we need to see how sensitive is the cost function the output $a^{(L-1)}$ of the previous layer:\n",
    "\n",
    "\n",
    "$$ \\frac{dC_0}{da^{(L-1)}} = \\frac{dz^{(L)}}{da^{(L-1)}} \\frac{da^{(L)}}{dz^{(L)}} \\frac{dC_0}{da^{(L)}}$$\n",
    "\n",
    "$$  \\frac{dz^{(L)}}{da^{(L-1)}} = \\frac{d(w^{(L)} a^{(L-1)} + b^{(L)})}{da^{(L-1)}} = w^{(L)}$$\n",
    "\n",
    "So, this is evaluated to:\n",
    "\n",
    "$$  \\boxed{\\frac{dC_0}{da^{(L-1)}} = w^{(L)} \\sigma '(z^{(L)}) 2(a^{(L)} - y)}  $$\n",
    "\n",
    "We can just keep iterating this same **chain rule** idea backwards to see how sensitive the cost function is to previous weights and biases.\n",
    "\n",
    "\n",
    "<center><img src=\"img/backprop_12.png\" alt=\"Backpropagation derivatives\" width=\"295\" height=\"538\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 14.</b> Backpropagation - Derive cost sensitivity to previous weights and biases</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will implement a simple network with 3 layers:\n",
    " * Input layer with 2 nodes\n",
    " * Hidden layer with 2 nodes\n",
    " * Output layer with 1 node\n",
    "\n",
    "The size of the dataset would be 33 rows with 2 features each (`years of experience` and `education level`) and 1 target value (`salary`).\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"img/backprop_13.png\" alt=\"Backpropagation implementation network\" width=\"485\" height=\"284\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 22.</b> Backpropagation - Simple neural network for our salary data</i></p>\n",
    "\n",
    "### Forward propagation\n",
    "$$Z^{(1)} = W^{(1)} X + B^{(1)}$$\n",
    "$$A^{(1)}1 = sigmoid(Z^{(1)})$$\n",
    "$$Z^{(2)} = W^{(2)} A^{(1)} + B^{(2)}$$\n",
    "$$A^{(2)} = sigmoid(Z^{(2)})$$\n",
    "\n",
    "\n",
    "### Backward propagation\n",
    "$$ Error = (A^{(2)} - Y) ^ 2$$\n",
    "$$ dA^{(2)} = 2 (A^{(2)} - Y) $$\n",
    "$$ dZ^{(2)} = sigmoid'(A^{(2)}) dA^{(2)} $$\n",
    "$$ dW^{(2)} = \\frac{1}{n} dZ^{(2)} \\cdot A^{(1)^T} $$\n",
    "$$ dB^{(2)} = \\frac{1}{n} \\sum dZ^{(2)} $$\n",
    "\n",
    "$$ dA^{(1)} = W^{(2)^T} \\cdot dZ^{(2)}  $$\n",
    "$$ dZ^{(1)} = sigmoid'(A^{(1)}) dA^{(1)} $$\n",
    "$$ dW^{(1)} = \\frac{1}{n} dZ^{(1)} \\cdot A^{(0)} $$\n",
    "\n",
    "### Variables and its shapes\n",
    "$$A^{(0)} = X : 33 \\times 2 $$\n",
    "$$Z^{(1)}, A^{(1)} : 2 \\times 33 $$\n",
    "$$Z^{(2)}, A^{(2)}, dA^{(2)}, dZ^{(2)}: 1 \\times 33 $$\n",
    "$$W^{(2)}, dW^{(2)}: 1 \\times 2 $$\n",
    "$$B^{(2)}, dB^{(2)}: 1 \\times 1 $$\n",
    "\n",
    "$$Z^{(1)}, A^{(1)}, dA^{(1)}, dZ^{(1)}: 2 \\times 33 $$\n",
    "$$W^{(1)}, dW^{(1)}: 2 \\times 2 $$\n",
    "$$B^{(1)}, dB^{(1)}: 2 \\times 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = years of experience\n",
    "X1 = [1.2, 1.3, 1.5, 1.8, 2, 2.1, 2.2, 2.5, 2.8, 2.9, 3.1, 3.3, 3.5, 3.8, 4, 4.1, 4.5, 4.9, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 10, 11, 12, 13, 14, 15]\n",
    "# X2 = level of education\n",
    "X2 = [2, 5, 3, 5, 3, 4, 2, 3, 4, 4, 3, 7, 5, 6, 5, 5, 2, 3, 4, 5, 6, 7, 5, 3, 2, 4, 5, 7, 3, 5, 7, 7, 5]\n",
    "# Y = salary\n",
    "Y = [2900, 3300, 3100, 4200, 3500, 3800, 3300, 3500, 3750, 4000, 3900, 5300, 4420, 5000, 4900, 5200, 3900, 4800, 5700, 6500, 6930, 7500, 7360, 6970, 6800, 7500, 8000, 9500, 11000, 9500, 12300, 13700, 12500]\n",
    "\n",
    "# Pandas data frame works with vectorized arrays (33 arrays of 1 element each)\n",
    "vectorized_X1 = np.array(X1).reshape(-1, 1)\n",
    "vectorized_X2 = np.array(X2).reshape(-1, 1)\n",
    "Y_train = np.array(Y).reshape(-1, 1) / 20000  # We divide by 20000 to scale down the output (it must be between 0 and 1)\n",
    "\n",
    "# Pack the train set\n",
    "X_train = np.concatenate([vectorized_X1, vectorized_X2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        # Initialize weights and biases using random numbers\n",
    "        self.weights = [np.random.randn(hidden_size, input_size), np.random.randn(output_size, hidden_size)]\n",
    "        self.biases = [np.zeros((hidden_size, 1)), np.zeros((output_size, 1))]\n",
    "\n",
    "    def sigmoid(self, X: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def sigmoid_derivative(self, X: np.ndarray) -> np.ndarray:\n",
    "        return X * (1 - X)\n",
    "\n",
    "    def forward(self, X: np.ndarray, print_values: bool = False) -> list[np.ndarray]:\n",
    "        z1 = self.weights[0] @ X.T + self.biases[0]\n",
    "        a1 = self.sigmoid(z1)\n",
    "\n",
    "        z2 = self.weights[1] @ a1 + self.biases[1]\n",
    "        a2 = self.sigmoid(z2)\n",
    "\n",
    "        return [z1, z2], [a1, a2]\n",
    "\n",
    "    def backward(self, X: np.ndarray, Y: np.ndarray, a: list[np.ndarray]):\n",
    "        a1 = a[0]\n",
    "        a2 = a[1]\n",
    "\n",
    "        # Calculate the error\n",
    "        error = (a2 - Y.T) ** 2\n",
    "        n = len(Y)\n",
    "\n",
    "        dc_da2 = 2 * (a2 - Y.T)                             # Derivative of the error w.r.t. the activation\n",
    "        dc_dz2 = self.sigmoid_derivative(a2) * dc_da2       # Derivative of the error w.r.t. the delta of the output layer\n",
    "        dc_dw2 = np.dot(dc_dz2, a1.T) / n                   # Derivative of the error w.r.t. the weights of the output layer\n",
    "        dc_db2 = np.sum(dc_dz2, axis=1, keepdims=True) / n  # Derivative of the error w.r.t. the biases of the output layer\n",
    "\n",
    "        dc_da1 = self.weights[1].T @ dc_dz2                 # Derivative of the error w.r.t. the activation of the hidden layer\n",
    "        dc_dz1 = self.sigmoid_derivative(a1) * dc_da1       # Derivative of the error w.r.t. the delta of the hidden layer\n",
    "        dc_dw1 = np.dot(dc_dz1, X) / n                      # Derivative of the error w.r.t. the weights of the hidden layer\n",
    "        dc_db1 = np.sum(dc_dz1, axis=1, keepdims=True) / n  # Derivative of the error w.r.t. the biases of the hidden layer\n",
    "\n",
    "        return dc_dw1, dc_dw2, dc_db1, dc_db2, error\n",
    "\n",
    "    def update_weights_and_biases(self, dc_dw1: np.ndarray, dc_dw2: np.ndarray, dc_db1: np.ndarray, dc_db2: np.ndarray, learning_rate: float):\n",
    "        self.weights[0] -= learning_rate * dc_dw1\n",
    "        self.weights[1] -= learning_rate * dc_dw2\n",
    "        self.biases[0] -= learning_rate * dc_db1\n",
    "        self.biases[1] -= learning_rate * dc_db2\n",
    "    \n",
    "    def train(self, X: np.ndarray, Y: np.ndarray, learning_rate: float, epochs: int):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            z, a = self.forward(X)\n",
    "\n",
    "            dc_dw1, dc_dw2, dc_db1, dc_db2, error = self.backward(X, Y, a)\n",
    "            losses.append(np.sum(error))\n",
    "\n",
    "            # Update the weights and biases\n",
    "            self.update_weights_and_biases(dc_dw1, dc_dw2, dc_db1, dc_db2, learning_rate)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch} error: {round(np.sum(error), 3)}\")\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 error: 2.573\n",
      "Epoch 1000 error: 0.06\n",
      "Epoch 2000 error: 0.025\n",
      "Epoch 3000 error: 0.022\n",
      "Epoch 4000 error: 0.021\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network = NeuralNetwork(2, 2, 1)  # 2 input nodes, 2 hidden nodes, 1 output node\n",
    "learning_rate = 0.5\n",
    "epochs = 5000\n",
    "losses = network.train(X_train, Y_train, learning_rate, epochs)\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network with 2 neurons in the hidden layer and 1 neuron in the output layer was constructed. The network was trained for `5000` epochs with a learning rate of `0.5`. These parameters were found with a little trial and error.\n",
    "\n",
    "Running the example prints the accumulated prediction error. Obviously, the backpropagation works and minimze the error with each epoch going down to `0.021`, which is not bad at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
