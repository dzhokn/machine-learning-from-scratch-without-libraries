{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous article we covered **Batch Gradient Descent**. It trains the model feeding it with **ALL** the training examples **AT ONCE**. \n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "As you can imagine, for large datasets, this can be quite computationally intensive and time-consuming. This is where **Stochastic Gradient Descent** comes into play. Instead of using the entire dataset to calculate the gradient, SGD randomly selects just one training example to compute the gradient in each iteration and the parameters ($w_{1..n}$ and $b$) are updated **after each example**.\n",
    "\n",
    "Think of this process as if you were again descending a mountain, but this time in thick fog with limited visibility. Rather than viewing the entire landscape to decide your next step, you make your decision based on where your foot lands next. This step is small and random, but it’s repeated many times, each time adjusting your path slightly in response to the immediate terrain under your feet.\n",
    "\n",
    "#### Advantages\n",
    "This stochastic nature of the algorithm provides several benefits:\n",
    "\n",
    "* **Speed**: By using only a small subset of data at a time, SGD can make rapid progress in reducing the loss, especially for large datasets.\n",
    "\n",
    "* **Escape from Local Minima**: The randomness helps SGD to potentially escape local minima, a common problem in complex optimization problems.\n",
    "\n",
    "* **Online Learning**: SGD is well-suited for online learning, where the model needs to be updated as new data comes in, due to its ability to update the model incrementally.\n",
    "\n",
    "#### Disadvantages\n",
    "However, the stochastic nature also introduces variability in the path to convergence. The algorithm doesn’t smoothly descend towards the minimum; rather, it takes a more zigzag path, which can sometimes make the convergence process appear erratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate Y\n",
    "Again, we need to create a function that calculates a $y$ first.\n",
    "\n",
    "$$ y = b + \\vec{w} \\cdot \\vec{x} $$\n",
    "\n",
    "where the size of $ \\vec{w}, \\vec{x} $ is all the same ($n$).\n",
    "\n",
    "$$ n - \\text{number of features} $$\n",
    "$$ m - \\text{number of observations}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_y(x: list[float], w: list[float], b: float) -> list[float]:\n",
    "    '''\n",
    "    Calculate the predicted value for a single observation.\n",
    "\n",
    "    Args:\n",
    "        x -> list[float]    : list of features (x1, x2, ..., xn)\n",
    "        w -> list[float]    : list of weights (w1, w2, ..., wn)\n",
    "        b -> float          : bias\n",
    "\n",
    "    Returns:\n",
    "        y -> float          : predicted value for a single observation\n",
    "    '''\n",
    "    n = len(x) # Number of features\n",
    "    y = b\n",
    "    for k in range(n):\n",
    "        y += w[k] * x[k] # y = w1*x1 + w2*x2 + ... + wn*xn + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate J (cost for single observation)\n",
    "Now, we need to calculate the cost (the delta between predicted $\\hat{y}$ and actual $y$). But just for a single observation and not for the whole set (i.e. no sum here).\n",
    "$$ J^{(i)} = (\\hat{y}^{(i)} - y^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(y: float, x: list[float], w: list[float], b: float) -> float:\n",
    "    '''\n",
    "    Calculate the cost by comparing the predicted and actual value for SINGLE observation.\n",
    "\n",
    "    Args:\n",
    "        y -> float              : expected float value (y)\n",
    "        x -> list[float]        : n features (x1, x2, ..., xn)\n",
    "        w -> list[float]        : n weights (w1, w2, ..., wn)\n",
    "        b -> float              : bias\n",
    "\n",
    "    Returns:\n",
    "        cost -> float           : cost for this observation\n",
    "    '''\n",
    "    y_pred = calculate_y(x, w, b) # Calculate the predicted value for the this observation\n",
    "    return (y_pred - y) ** 2 # Calculate the squared difference between the predicted and actual value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate dJ/dw and dJ/db (cost derivatives)\n",
    "We need to calculate the derivative of $J^{(i)}$ - once w.r.t to $w$ and once w.r.t to $b$. This way we know how strongly a change in $w$ and $b$ impacts $J^{(i)}$.\n",
    "\n",
    "$$ \\frac{dJ^{(i)}}{dw_k} = 2 (\\hat{y}^{(i)}  - y^{(i)}) x^{(i)}_k  $$\n",
    "$$ \\frac{dJ^{(i)}}{db} = 2 (\\hat{y}^{(i)}  - y^{(i)}) $$\n",
    "\n",
    "Where $i$ is the index of the particular observation we are using for executing the gradient descent and $k$ is the index of the feature. Note that for each feature ($n$ in total) we calculate a separate $\\frac{dJ^{(i)}}{dw_k}$ derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dj_dw(y: float, x: list[float], w: list[float], b: float) -> list[float]:\n",
    "    '''\n",
    "    Calculate the derivative of the cost function with respect to each weight.\n",
    "\n",
    "    Args:\n",
    "        y -> float              : expected float value (y)\n",
    "        x -> list[float]        : n features (x1, x2, ..., xn)\n",
    "        w -> list[float]        : n weights (w1, w2, ..., wn)\n",
    "        b -> float              : bias\n",
    "\n",
    "    Returns:\n",
    "        dj_dw -> list[float]    : n weight derivatives (dj_dw1, dj_dw2, ..., dj_dwn)\n",
    "    '''\n",
    "    n = len(w) # Number of features\n",
    "    dj_dw = [0] * n # Initialize the derivative of the cost function with respect to the weights. n-sized list of zeros.\n",
    "\n",
    "    y_pred = calculate_y(x, w, b) # Calculate the predicted value for the current observation\n",
    "    for j in range(n): # For each feature\n",
    "        dj_dw[j] += 2 * (y_pred - y) * x[j] # Calculate the derivative of the cost function with respect to the weights\n",
    "\n",
    "    return dj_dw\n",
    "\n",
    "def calculate_dj_db(y: float, x: list[float], w: list[float], b: float) -> float:\n",
    "    '''\n",
    "    Calculate the derivative of the cost function with respect to the bias.\n",
    "\n",
    "    Args:\n",
    "        y -> float              : expected float value (y)\n",
    "        x -> list[float]        : n features (x1, x2, ..., xn)\n",
    "        w -> list[float]        : n weights (w1, w2, ..., wn)\n",
    "        b -> float              : bias\n",
    "\n",
    "    Returns:\n",
    "        dj_db -> float          : derivative of the cost function with respect to the bias\n",
    "    '''\n",
    "    y_pred = calculate_y(x, w, b) # Calculate the predicted value for the current observation\n",
    "    dj_db = 2 * (y_pred - y) # Calculate the derivative of the cost function with respect to the bias\n",
    "    return dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Descent\n",
    "Once we know how each weight affects the cost we can slightly update these weights in the direction we want (up or down), based on the derivative result (negative or positive). \n",
    "\n",
    "The term ‘stochastic’ refers to a system or process that is linked with a **random** probability. In each iteration of the training process, SGD randomly selects a single data point from the entire dataset. This randomness is what makes it **‘stochastic’**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X: list[list[float]], Y: list[float], w: list[float], b: float, learn_rate: float, num_iterations: int) -> tuple[list[float], float]:\n",
    "    '''\n",
    "    Perform stochasticgradient descent to find the optimal weights and bias.\n",
    "\n",
    "    Args:\n",
    "        X -> list[list[float]]  : m rows of n features each (x^1, x^2, ..., x^m)\n",
    "        Y -> list[float]        : m rows of expected float values (y^1, y^2, ..., y^m)\n",
    "        w -> list[float]        : n weights (w1, w2, ..., wn)\n",
    "        b -> float              : bias\n",
    "        learn_rate -> float     : learning rate\n",
    "        num_iterations -> int   : number of iterations to perform\n",
    "\n",
    "    Returns:\n",
    "        w -> list[float]        : n weights (w1, w2, ..., wn)\n",
    "        b -> float              : bias\n",
    "    '''\n",
    "    for iteration_number in range(num_iterations):\n",
    "\n",
    "        # Randomize the order of the observations\n",
    "        m = len(X)\n",
    "        random_indices = np.random.permutation(m)\n",
    "        X_shuffled = X[random_indices]\n",
    "        Y_shuffled = Y[random_indices]\n",
    "\n",
    "        for i in range(m):\n",
    "            x = X_shuffled[i] # Get a single observation\n",
    "            y = Y_shuffled[i] # Get the expected value for that observation\n",
    "\n",
    "            dj_dw = calculate_dj_dw(y, x, w, b) # Calculate the derivative of the cost function with respect to the weights\n",
    "            dj_db = calculate_dj_db(y, x, w, b) # Calculate the derivative of the cost function with respect to the bias\n",
    "\n",
    "            # Update the weights and bias\n",
    "            w = [w[j] - learn_rate * dj_dw[j] for j in range(len(w))]\n",
    "            b = b - learn_rate * dj_db\n",
    "\n",
    "        # Print the cost every 100 iterations\n",
    "        if iteration_number % 1000 == 0:\n",
    "            print(f\"Iteration {iteration_number}: Total cost = {calculate_total_cost(X, Y, w, b)}\")\n",
    "    return w, b\n",
    "\n",
    "def calculate_total_cost(X: np.ndarray, Y: np.ndarray, w: list[float], b: float) -> float:\n",
    "    '''Calculate the total cost by comparing the predicted and actual values for ALL observations.'''\n",
    "    m = len(Y) # Number of observations\n",
    "    cost = 0\n",
    "    for i in range(m):                      # For each observation\n",
    "        y_pred_i = X[i] @ w + b             # Calculate the predicted value for the current observation\n",
    "        cost += (y_pred_i - Y[i]) ** 2      # Calculate the squared difference between the predicted and actual values\n",
    "    return cost / m                         # Normalize by the number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Demo\n",
    "Let's do the well-known demo with **salary prediction**, where `salary` ($y$) is predicted by using `year of experience` ($x_1$) and `education level` ($x_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = years of experience\n",
    "X1 = [1.2, 1.3, 1.5, 1.8, 2, 2.1, 2.2, 2.5, 2.8, 2.9, 3.1, 3.3, 3.5, 3.8, 4, 4.1, 4.5, 4.9, 5, 5.5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 10, 11, 12, 13, 14, 15]\n",
    "# X2 = level of education \n",
    "X2 = [2, 5, 3, 5, 3, 4, 2, 3, 4, 4, 3, 7, 5, 6, 5, 5, 2, 3, 4, 5, 6, 7, 5, 3, 2, 4, 5, 7, 3, 5, 7, 7, 5]\n",
    "# Y = salary\n",
    "Y = [2900, 3300, 3100, 4200, 3500, 3800, 3300, 3500, 3750, 4000, 3900, 5300, 4420, 5000, 4900, 5200, 3900, 4800, 5700, 6500, 6930, 7500, 7360, 6970, 6800, 7500, 8000, 9500, 11000, 9500, 12300, 13700, 12500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Total cost = 43011296.25774596\n",
      "Iteration 1000: Total cost = 318186.91040476674\n",
      "Iteration 2000: Total cost = 309101.7593776509\n",
      "Iteration 3000: Total cost = 301190.1557361399\n",
      "Iteration 4000: Total cost = 294300.09965818824\n",
      "Iteration 5000: Total cost = 288300.2367203786\n",
      "Iteration 6000: Total cost = 283074.57090358174\n",
      "Iteration 7000: Total cost = 278524.21185012016\n",
      "Iteration 8000: Total cost = 274561.4072313068\n",
      "Iteration 9000: Total cost = 271110.522816652\n",
      "Iteration 10000: Total cost = 268105.09922716545\n",
      "Iteration 11000: Total cost = 265488.051391522\n",
      "Iteration 12000: Total cost = 263208.72562447545\n",
      "Iteration 13000: Total cost = 261223.98199592295\n",
      "Iteration 14000: Total cost = 259495.34535411824\n",
      "Iteration 15000: Total cost = 257990.20651471458\n",
      "Iteration 16000: Total cost = 256679.23133047236\n",
      "Iteration 17000: Total cost = 255537.72665836426\n",
      "Iteration 18000: Total cost = 254543.42066155915\n",
      "Iteration 19000: Total cost = 253677.64594415313\n",
      "Iteration 20000: Total cost = 252923.38491598426\n",
      "Iteration 21000: Total cost = 252266.69030004428\n",
      "Iteration 22000: Total cost = 251694.82221801733\n",
      "Iteration 23000: Total cost = 251196.6082514494\n",
      "Iteration 24000: Total cost = 250762.775834186\n",
      "Iteration 25000: Total cost = 250385.0072983082\n",
      "Iteration 26000: Total cost = 250056.14182990682\n",
      "Iteration 27000: Total cost = 249769.72454899407\n",
      "Iteration 28000: Total cost = 249520.05881204424\n",
      "Iteration 29000: Total cost = 249302.90487595802\n",
      "Iteration 30000: Total cost = 249113.31509583202\n",
      "Iteration 31000: Total cost = 248948.53334805433\n",
      "Iteration 32000: Total cost = 248804.92600178038\n",
      "Iteration 33000: Total cost = 248679.93794180948\n",
      "Iteration 34000: Total cost = 248571.0081027847\n",
      "Iteration 35000: Total cost = 248476.11395883438\n",
      "Iteration 36000: Total cost = 248393.42680616843\n",
      "Iteration 37000: Total cost = 248321.53728541607\n",
      "Iteration 38000: Total cost = 248258.7991739503\n",
      "Iteration 39000: Total cost = 248204.19366158787\n",
      "Iteration 40000: Total cost = 248156.6226229374\n",
      "Iteration 41000: Total cost = 248115.1971340356\n",
      "Iteration 42000: Total cost = 248079.13661983848\n",
      "Iteration 43000: Total cost = 248047.73929448362\n",
      "Iteration 44000: Total cost = 248020.32160171357\n",
      "Iteration 45000: Total cost = 247996.48354563795\n",
      "Iteration 46000: Total cost = 247975.75796950626\n",
      "Iteration 47000: Total cost = 247957.6976769968\n",
      "Iteration 48000: Total cost = 247941.9314901008\n",
      "Iteration 49000: Total cost = 247928.11878429103\n",
      "Iteration 50000: Total cost = 247916.1262164665\n",
      "Iteration 51000: Total cost = 247905.7068781554\n",
      "Iteration 52000: Total cost = 247896.61036399877\n",
      "Iteration 53000: Total cost = 247888.85841661663\n",
      "Iteration 54000: Total cost = 247881.8498107383\n",
      "Iteration 55000: Total cost = 247875.86353722904\n",
      "Iteration 56000: Total cost = 247870.5399030513\n",
      "Iteration 57000: Total cost = 247865.99624550363\n",
      "Iteration 58000: Total cost = 247862.0028869343\n",
      "Iteration 59000: Total cost = 247858.53947048035\n",
      "Iteration 60000: Total cost = 247855.50030382286\n",
      "Iteration 61000: Total cost = 247852.87285829804\n",
      "Iteration 62000: Total cost = 247850.6072776377\n",
      "Iteration 63000: Total cost = 247848.5779204528\n",
      "Iteration 64000: Total cost = 247846.98171709772\n",
      "Iteration 65000: Total cost = 247845.31374678182\n",
      "Iteration 66000: Total cost = 247844.01416073838\n",
      "Iteration 67000: Total cost = 247842.8372346871\n",
      "Iteration 68000: Total cost = 247841.81596842234\n",
      "Iteration 69000: Total cost = 247841.0245413772\n",
      "Iteration 70000: Total cost = 247840.1884145365\n",
      "Iteration 71000: Total cost = 247839.60203801034\n",
      "Iteration 72000: Total cost = 247838.98659540428\n",
      "Iteration 73000: Total cost = 247838.50024879337\n",
      "Iteration 74000: Total cost = 247838.08353659013\n",
      "Iteration 75000: Total cost = 247837.56217543132\n",
      "Iteration 76000: Total cost = 247837.2140206084\n",
      "Iteration 77000: Total cost = 247836.97448874175\n",
      "Iteration 78000: Total cost = 247836.6551691015\n",
      "Iteration 79000: Total cost = 247836.42267721018\n",
      "Iteration 80000: Total cost = 247836.29912794035\n",
      "Iteration 81000: Total cost = 247836.06310553182\n",
      "Iteration 82000: Total cost = 247835.9616121049\n",
      "Iteration 83000: Total cost = 247835.7627691556\n",
      "Iteration 84000: Total cost = 247835.72972931355\n",
      "Iteration 85000: Total cost = 247835.569584025\n",
      "Iteration 86000: Total cost = 247835.46330614557\n",
      "Iteration 87000: Total cost = 247835.5780355051\n",
      "Iteration 88000: Total cost = 247835.49619328522\n",
      "Iteration 89000: Total cost = 247835.33232217934\n",
      "Iteration 90000: Total cost = 247835.2094643822\n",
      "Iteration 91000: Total cost = 247835.21379849105\n",
      "Iteration 92000: Total cost = 247835.1732945785\n",
      "Iteration 93000: Total cost = 247835.0783225198\n",
      "Iteration 94000: Total cost = 247835.1140641185\n",
      "Iteration 95000: Total cost = 247835.1018927248\n",
      "Iteration 96000: Total cost = 247834.9998406463\n",
      "Iteration 97000: Total cost = 247835.02930513414\n",
      "Iteration 98000: Total cost = 247835.0474953796\n",
      "Iteration 99000: Total cost = 247834.9395111834\n",
      "Final `b` is 954.4356218155173 and `w` are [np.float64(691.416369338105), np.float64(287.78573352311645)]\n"
     ]
    }
   ],
   "source": [
    "# Merge the X1 and X2 into a single X\n",
    "X = [[x1, x2] for x1, x2 in zip(X1, X2)]\n",
    "\n",
    "# Convert the X, Y to a numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Configure gradient descent settings\n",
    "b_init = 0          # initial bias (could be random as well)\n",
    "w_init = [0, 0]     # initial weights (could be random as well)\n",
    "learn_rate = 0.00001   # learning rate (step size)\n",
    "iterations = 100000   # number of iterations (epochs)\n",
    "\n",
    "# Execute the gradient descent in search for optimal cost function (optimal `w` and `b`)\n",
    "w, b = gradient_descent(X, Y, w_init, b_init, learn_rate, iterations)\n",
    "print(f\"Final `b` is {b} and `w` are {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The final results in terms of cost and weights are basically the same as in the BGD. Yet, the stochastic gradient descent executes its training epochs quite faster. \n",
    "\n",
    "However, as you can see it requires **20x** more epochs of training in comparison with the BGD, so the speed factor is basically neutralized:\n",
    "* `100 000` vs `5 000` epochs\n",
    "\n",
    "Another observation is that in order to have stable loss minimization curve we need a significantly smaller learning rate here:\n",
    "* `0.00001` vs `0.01` learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* [The Math Behind Stochastic Gradient Descent](https://towardsdatascience.com/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
