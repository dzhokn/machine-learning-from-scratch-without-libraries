{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe84d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e1abf",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "We have briefly covered the decision trees in the previous chapter (*Regression Tree*).\n",
    "\n",
    "Now, we'll explain some advanced topics.\n",
    "\n",
    "## Splitting criteria\n",
    "In a Decision Tree, the process of splitting data at each node is important. The splitting criteria finds the best feature to split the data on. In the regressor chapter we used **variance reduction** for splitting mechanism. Other common splitting criteria include **Gini Impurity** and **Entropy**.\n",
    "\n",
    "#### Gini Impurity\n",
    "This criterion measures how \"impure\" a node is. **Gini Impurity** represents the probability of **incorrectly classifying** a randomly chosen element if it were labeled according to the distribution of labels in the subset. The lower the Gini Impurity, the better the feature splits the data into distinct categories.\n",
    "\n",
    "To calculate Gini impurity, use the formula:\n",
    "$$ \\text{Gini Impurity} = 1 - \\sum{(p_i)^2}$$\n",
    "where $p_i$ is the proportion of class $i$ in a node. For example, if a node has $5$ \"Yes\" and $5$ \"No\" samples, the Gini impurity is:\n",
    "$$1 - (\\frac{5}{10})^2 - (\\frac{5}{10})^2 = 0.5$$\n",
    "indicating a $50$% chance of misclassification. A perfectly pure node with all \"Yes\" samples has a Gini impurity of:\n",
    "$$1 - (1)^2 - (0)^2= 0$$\n",
    "\n",
    "In general the formula is:\n",
    "$$Gini(p_1, p_2) = 1 - p_1^2 - p_2^2$$\n",
    "\n",
    "<center><img src=\"img/gini_impurity.png\" alt=\"Gini impurity\" width=\"800\" height=\"567\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 1.</b> 0.394 is high impurity. It says there is a mix of classes in that tree and its subtrees. </i></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55a7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(Y: np.ndarray):\n",
    "    '''\n",
    "    Calculate the GINI INDEX for a given target variable.\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): The column vector of the target variable.\n",
    "\n",
    "    Returns:\n",
    "        float: The Gini index.\n",
    "    '''\n",
    "    # Extract all unique classes in the target variable\n",
    "    class_labels = np.unique(Y)\n",
    "    gini = 0\n",
    "    # For each class, calculate the proportion of samples that belong to that class\n",
    "    for cls in class_labels:\n",
    "        # Calculate the proportion of samples that belong to the current class to all samples in the dataset\n",
    "        p_cls = len(Y[Y == cls]) / len(Y)\n",
    "        # Sum the squared proportions of each class\n",
    "        gini += p_cls**2\n",
    "    # Return 1 minus the sum of the squared proportions of each class (as per the formula)\n",
    "    return 1 - gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020e697",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "This measures the **amount of uncertainty or randomness** in the data. The tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable.\n",
    "These criteria help decide which features are useful for making the best split at each decision point in the tree.\n",
    "\n",
    "* If a dataset contains examples from only *one class*, its **entropy is zero**, indicating complete purity.\n",
    "* If the dataset is **evenly split** between classes, its **entropy is at its maximum** which is 1, indicating maximum randomness.\n",
    "\n",
    "The formula for entropy (H) for a binary classification problem is:\n",
    "$$H(p_1, p_2) = - p_1 log_2(p_1) - p_2 log_2(p_2)$$\n",
    "where $p_1$ and $p_2$ are the probabilities of two classes.\n",
    "\n",
    "\n",
    "<center><img src=\"img/entropy_1.png\" alt=\"How is entropy measured?\" width=\"900\" height=\"491\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 2.</b> The entropy of a binary classification problem is in [0, 1] range</i></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d33d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(Y: np.ndarray):\n",
    "    '''\n",
    "    Calculate the ENTROPY for a given target variable.\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): The column vector of the target variable.\n",
    "\n",
    "    Returns:\n",
    "        float: The entropy.\n",
    "    '''\n",
    "\n",
    "    class_labels = np.unique(Y)\n",
    "    entropy = 0\n",
    "    for cls in class_labels:\n",
    "        p_cls = len(Y[Y == cls]) / len(Y)\n",
    "        entropy += -p_cls * np.log2(p_cls) # It is almost the same as Gini impurity (extra logarithmic calculation is added)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa280c60",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "As you can see the formulas are quite similar. Thus, they lead to similar results. That's why people mostly prefer to use **Gini**, since they want to avoid the logarithmic calculation (which is relatively slow operation).\n",
    "\n",
    "|  | Gini | Entropy |\n",
    "| --- | --- | --- |\n",
    "| **Measure** | Probability of misclassifying a random input | The amount of uncertainty (randomness) in a set |\n",
    "| **Range** | $[0, 0.5]$ | $[0, log2(C)]$, where $C$ is the number of classes. For binary classification it is $[0, 1]$ |\n",
    "| **Interpretation** | The expected error rate in a classifier. | The average amount of information needed to specify the class of an instance. |\n",
    "| **Sensitivity** | It is sensitive to the distribution of classes in a set. | It is sensitive to the number of classes. |\n",
    "| **Bias** | It has a bias toward selecting splits that result in a more balanced distribution of classes. | It has a bias toward selecting splits that result in a higher reduction of uncertainty. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50dd61",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "* Pruning is an important technique used to **prevent overfitting** in Decision Trees. Overfitting occurs when a tree becomes too deep and starts to memorize the training data rather than learning general patterns. This leads to poor performance on new, unseen data.\n",
    "* This technique reduces the complexity of the tree by **removing branches** that have **little predictive power**. It improves model performance by helping the tree generalize better to new data. It also makes the model simpler and faster to deploy.\n",
    "* It is useful when a Decision Tree is too deep and starts to capture noise in the data.\n",
    "\n",
    "There are two main types of decision tree pruning: **Pre-Pruning** and **Post-Pruning**.\n",
    "\n",
    "#### Pre-Pruning (Early Stopping)\n",
    "Sometimes, the growth of the decision tree can be stopped before it gets too complex, this is called pre-pruning. It is important to prevent the overfitting of the training data, which results in a poor performance when exposed to new data.\n",
    "\n",
    "Some common pre-pruning techniques include:\n",
    "\n",
    "* **Maximum Depth**: It limits the maximum level of depth in a decision tree.\n",
    "* **Minimum Samples per Leaf**: Set a minimum threshold for the number of samples in each leaf node.\n",
    "* **Minimum Samples per Split**: Specify the minimal number of samples needed to break up a node.\n",
    "* **Maximum Features**: Restrict the quantity of features considered for splitting.\n",
    "\n",
    "#### Post-Pruning (Reducing Nodes)\n",
    "After the­ tree is fully grown, post-pruning involves re­moving branches or nodes to improve the­ model's ability to generalize­. Some common post-pruning techniques include­:\n",
    "\n",
    "* **Cost-Complexity Pruning (CCP)**: This method assigns a price to each subtre­e primarily based on its *accuracy* and *complexity*, the­n selects the subtre­e with the *lowest fee*.\n",
    "* **Re­duced Error Pruning**: Removes branche­s that do not significantly affect the overall accuracy.\n",
    "* **Minimum Impurity De­crease**: Prunes node­s if the decrease­ in impurity (Gini impurity or entropy) is beneath a ce­rtain threshold.\n",
    "* **Minimum Leaf Size**: Re­moves leaf nodes with fe­wer samples than a specifie­d threshold.\n",
    "\n",
    "Post-pruning simplifies the tre­e while preserving its Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8754212",
   "metadata": {},
   "source": [
    "# Classification Tree\n",
    "\n",
    "It is the same as the **regression tree**. The **only difference** is that for information gain we will not be using **variance_reduction** method, but **Gini index** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3ccecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(parent: np.ndarray, l_child: np.ndarray , r_child: np.ndarray):\n",
    "    ''' \n",
    "    In regression tree variance reduction is used. In classification tasks we use \n",
    "    Gini Index or Entropy to calculate the information gain.\n",
    "    '''\n",
    "    weight_l = len(l_child) / len(parent)\n",
    "    weight_r = len(r_child) / len(parent)\n",
    "    return gini_index(parent) - (weight_l*gini_index(l_child) + weight_r*gini_index(r_child))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe91505",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795f012",
   "metadata": {},
   "source": [
    "#### Node\n",
    "\n",
    "The node implementation is the same as the one in the **regression tree**. However, instead of `var_red` we have `info_gain` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285780bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,\n",
    "    feature_index: int = None,\n",
    "    threshold: float = None,\n",
    "    info_gain: float = None,\n",
    "    left: 'Node' = None,\n",
    "    right: 'Node' = None,\n",
    "    value: float = None):\n",
    "        '''\n",
    "        Constructor for the Node class.\n",
    "\n",
    "        Args:\n",
    "            feature_index (int):    The index of the feature to split on.\n",
    "            threshold (float):      The threshold value to split on.\n",
    "            info_gain (float):      The information gain achieved by the split.\n",
    "            left (Node):            The left child node.\n",
    "            right (Node):           The right child node.\n",
    "            value (float):          The value of the node (in case of a leaf node).\n",
    "        ''' \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.info_gain = info_gain\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98e4e7",
   "metadata": {},
   "source": [
    "#### Tree\n",
    "\n",
    "Now, let's build the tree. The magic here happens in `get_best_split` method. There we evaluate candidate splits and rank them by gini index. The one with highest gini becomes a new decision node in our tree. Then we go deeper, until we hit a tree `max_depth` limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f983d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree():\n",
    "    def __init__(self, min_samples_split: int = 2, max_depth: int = 3):\n",
    "        '''\n",
    "        Constructor for the RegressionTree class.\n",
    "\n",
    "        Args:\n",
    "            min_samples_split (int): The minimum number of samples required to split an internal node.\n",
    "            max_depth (int):         The maximum depth of the tree.\n",
    "        '''\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def build_tree(self, dataset: np.ndarray, curr_depth: int = 0) -> Node:\n",
    "        '''\n",
    "        Recrusively build the decision tree.\n",
    "\n",
    "        Args:\n",
    "            dataset (np.ndarray):   The dataset to build the tree on.\n",
    "            curr_depth (int):       The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "            Node: The root node of the tree.\n",
    "        '''\n",
    "        # Split the dataset into features and target variable\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        best_split = {}\n",
    "\n",
    "        # Split until stopping conditions are met (pre-pruning criteria):\n",
    "        # 1. Minimum number of samples to split an internal node\n",
    "        # 2. Maximum depth of the tree\n",
    "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                # Recursively build the left subtree\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # Recursively build the right subtree\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # CASE 1: Return a new decision node (root node for the two new subtrees)\n",
    "                return Node(best_split[\"feature_index\"],\n",
    "                            best_split[\"threshold\"],\n",
    "                            best_split[\"info_gain\"],\n",
    "                            left_subtree, \n",
    "                            right_subtree)\n",
    "        \n",
    "        # CASE 2: Otherwise, compute and return a leaf node.\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def get_best_split(self, \n",
    "            dataset: np.ndarray,\n",
    "            num_features: int) -> dict:\n",
    "        '''\n",
    "        Find the best split for the dataset. This happens by building a candidate split for each feature and each value of this feature.\n",
    "        So the complexity of this method is O(m x u), where m is the number of features and u is the avg number of unique values within a feature.\n",
    "        Wa rank the candidate splits by `information gain` (gini indexn) and always pick the one with highest `information gain`.\n",
    "\n",
    "        Args:\n",
    "            dataset (np.ndarray):   The dataset to find the best split on.\n",
    "            num_features (int):     The number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict: The best split.\n",
    "        '''\n",
    "        # Dictionary to store the best split\n",
    "        best_split = {}\n",
    "        # Start with an extremely small number.\n",
    "        max_info_gain = -float(\"inf\")\n",
    "\n",
    "        # Loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            # List of candidate thresholds - unique values of the feature.\n",
    "            threshold_candidates = np.unique(feature_values)\n",
    "            # Loop over all the feature values present in the data\n",
    "            for threshold_candidate in threshold_candidates:\n",
    "                # Build a candidate split.\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold_candidate)\n",
    "                # If childs are not empty.\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # Then, compute the information gain of the candidate split.\n",
    "                    # NB: THIS IS THE ONLY DIFFERENCE WITH THE REGRESSION TREE\n",
    "                    curr_info_gain = information_gain(y, left_y, right_y)\n",
    "                    # And update the best split if needed. The best split is the one \n",
    "                    # with the highest information gain (gini index).\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"]     = threshold_candidate\n",
    "                        best_split[\"dataset_left\"]  = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"]     = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "\n",
    "        # Return the best split (the one with the highest information gain).\n",
    "        return best_split\n",
    "\n",
    "    def split(self, \n",
    "            dataset: np.ndarray, \n",
    "            feature_index: int, \n",
    "            threshold: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "        ''' \n",
    "        Split the dataset into two subsets (candidate split) based on the feature values and threshold.\n",
    "\n",
    "        Args:\n",
    "            dataset (np.ndarray):   The dataset to split.\n",
    "            feature_index (int):    The index of the feature to split on.\n",
    "            threshold (float):      The threshold value to split on.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The candidate split (two subsets of the dataset).\n",
    "        '''\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def calculate_leaf_value(self, Y: np.ndarray) -> float:\n",
    "        ''' \n",
    "        Compute the leaf node value. Basically, the average of the target variable.\n",
    "\n",
    "        Args:\n",
    "            Y (np.ndarray): The target variable.\n",
    "        '''\n",
    "        val = np.mean(Y)\n",
    "        return val\n",
    "\n",
    "    def print_tree(self, tree = None, indent = \" \"):\n",
    "        ''' \n",
    "        Recursively print the decision tree.\n",
    "\n",
    "        Args:\n",
    "            tree (Node): The root node of the tree.\n",
    "            indent (str): The indentation string.\n",
    "        '''\n",
    "        # CASE 1: We are at root level\n",
    "        if not tree:\n",
    "            print(\"X_0 is yrs_of_XP, X_1 is education_level\")\n",
    "            print(\"------\")\n",
    "            tree = self.root\n",
    "\n",
    "        # CASE 2: We are at leaf level\n",
    "        if tree.value is not None:\n",
    "            print(f\"{tree.value:.0f}\")\n",
    "\n",
    "        # CASE 3: We are at internal node level\n",
    "        else:\n",
    "            info_gain_str = f\"{tree.info_gain:.3f}\"\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"? Info Gain: \", info_gain_str)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        ''' \n",
    "        Train the decision tree.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): The input features.\n",
    "            Y (np.ndarray): The target variable.\n",
    "        '''\n",
    "\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def make_prediction(self, x: np.ndarray, tree: Node) -> float:\n",
    "        '''\n",
    "        Recursively predict the target variable for a new dataset.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): A data point from the dataset.\n",
    "            tree (Node):    The root node of the tree.\n",
    "        '''\n",
    "        # CASE 1: Leaf node\n",
    "        if tree.value is not None: \n",
    "            return tree.value\n",
    "\n",
    "        # CASE 2: Decision node\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            # CASE 2.1: Go down the left child\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            # CASE 2.2: Go down the right child\n",
    "            return self.make_prediction(x, tree.right)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        ''' \n",
    "        Predict the target variable for a new dataset.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): All the input features from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray:     The predicted target variable.\n",
    "        '''\n",
    "        # For each data point in the dataset, make a prediction.\n",
    "        predictions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efcc15",
   "metadata": {},
   "source": [
    "# Usage\n",
    "#### School dropout prediction\n",
    "Now, we'll use our **Classification Tree** to predict school dropouts (pupils who fall out of school before completing their educational program). For that sake, we will use a popular [students dataset](https://www.kaggle.com/datasets/mahwiz/students-dropout-and-academic-success-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb2c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (4423, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance\\t</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Previous qualification (grade)</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>9773</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>8014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Marital status  Application mode  Application order  Course  \\\n",
       "0               1                17                  5     171   \n",
       "1               1                15                  1    9254   \n",
       "2               1                 1                  5    9070   \n",
       "3               1                17                  2    9773   \n",
       "4               2                39                  1    8014   \n",
       "\n",
       "   Daytime/evening attendance\\t  Previous qualification  \\\n",
       "0                             1                       1   \n",
       "1                             1                       1   \n",
       "2                             1                       1   \n",
       "3                             1                       1   \n",
       "4                             0                       1   \n",
       "\n",
       "   Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
       "0                           122.0            1                      19   \n",
       "1                           160.0            1                       1   \n",
       "2                           122.0            1                      37   \n",
       "3                           122.0            1                      38   \n",
       "4                           100.0            1                      37   \n",
       "\n",
       "   Father's qualification  ...  Curricular units 2nd sem (credited)  \\\n",
       "0                      12  ...                                    0   \n",
       "1                       3  ...                                    0   \n",
       "2                      37  ...                                    0   \n",
       "3                      37  ...                                    0   \n",
       "4                      38  ...                                    0   \n",
       "\n",
       "   Curricular units 2nd sem (enrolled)  \\\n",
       "0                                    0   \n",
       "1                                    6   \n",
       "2                                    6   \n",
       "3                                    6   \n",
       "4                                    6   \n",
       "\n",
       "   Curricular units 2nd sem (evaluations)  \\\n",
       "0                                       0   \n",
       "1                                       6   \n",
       "2                                       0   \n",
       "3                                      10   \n",
       "4                                       6   \n",
       "\n",
       "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
       "0                                    0                          0.000000   \n",
       "1                                    6                         13.666667   \n",
       "2                                    0                          0.000000   \n",
       "3                                    5                         12.400000   \n",
       "4                                    6                         13.000000   \n",
       "\n",
       "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
       "0                                               0               10.8   \n",
       "1                                               0               13.9   \n",
       "2                                               0               10.8   \n",
       "3                                               0                9.4   \n",
       "4                                               0               13.9   \n",
       "\n",
       "   Inflation rate   GDP    Target  \n",
       "0             1.4  1.74   Dropout  \n",
       "1            -0.3  0.79  Graduate  \n",
       "2             1.4  1.74   Dropout  \n",
       "3            -0.8 -3.12  Graduate  \n",
       "4            -0.3  0.79  Graduate  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "students_df = pd.read_csv(\"data/students_dataset.csv.gz\", compression=\"gzip\")\n",
    "print(f\"Shape of the dataset: {students_df.shape}\")\n",
    "students_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949204c",
   "metadata": {},
   "source": [
    "In this dataset we have 4423 student records. For each one of them we have information about the socio-economic status of the student and her parents. Furthermore, we have the target variable: has the student `Graduated` or `Dropped out` of school.\n",
    "\n",
    "#### Preprocess data\n",
    "\n",
    "We also have 800 students in `Enrolled` state. We will simply discard them for the sake of simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11079e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (3629, 37)\n"
     ]
    }
   ],
   "source": [
    "# Drop `Enrolled` status (we will operate only with Dropouts and Graduates)\n",
    "students_df = students_df[ (students_df[\"Target\"] != 'Enrolled')]\n",
    "print(f\"Shape of the dataset: {students_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8627aa2",
   "metadata": {},
   "source": [
    "Before we start our training we need to convert the labels (`Graduate` and `Dropout` into numeric values - $0$ and $1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8340c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzho\\AppData\\Local\\Temp\\ipykernel_17180\\3877963097.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  students_df.Target = students_df.Target.replace({\"Graduate\": 0, \"Dropout\": 1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance\\t</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Previous qualification (grade)</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>9773</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>8014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Marital status  Application mode  Application order  Course  \\\n",
       "0               1                17                  5     171   \n",
       "1               1                15                  1    9254   \n",
       "2               1                 1                  5    9070   \n",
       "3               1                17                  2    9773   \n",
       "4               2                39                  1    8014   \n",
       "\n",
       "   Daytime/evening attendance\\t  Previous qualification  \\\n",
       "0                             1                       1   \n",
       "1                             1                       1   \n",
       "2                             1                       1   \n",
       "3                             1                       1   \n",
       "4                             0                       1   \n",
       "\n",
       "   Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
       "0                           122.0            1                      19   \n",
       "1                           160.0            1                       1   \n",
       "2                           122.0            1                      37   \n",
       "3                           122.0            1                      38   \n",
       "4                           100.0            1                      37   \n",
       "\n",
       "   Father's qualification  ...  Curricular units 2nd sem (credited)  \\\n",
       "0                      12  ...                                    0   \n",
       "1                       3  ...                                    0   \n",
       "2                      37  ...                                    0   \n",
       "3                      37  ...                                    0   \n",
       "4                      38  ...                                    0   \n",
       "\n",
       "   Curricular units 2nd sem (enrolled)  \\\n",
       "0                                    0   \n",
       "1                                    6   \n",
       "2                                    6   \n",
       "3                                    6   \n",
       "4                                    6   \n",
       "\n",
       "   Curricular units 2nd sem (evaluations)  \\\n",
       "0                                       0   \n",
       "1                                       6   \n",
       "2                                       0   \n",
       "3                                      10   \n",
       "4                                       6   \n",
       "\n",
       "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
       "0                                    0                          0.000000   \n",
       "1                                    6                         13.666667   \n",
       "2                                    0                          0.000000   \n",
       "3                                    5                         12.400000   \n",
       "4                                    6                         13.000000   \n",
       "\n",
       "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
       "0                                               0               10.8   \n",
       "1                                               0               13.9   \n",
       "2                                               0               10.8   \n",
       "3                                               0                9.4   \n",
       "4                                               0               13.9   \n",
       "\n",
       "   Inflation rate   GDP  Target  \n",
       "0             1.4  1.74       1  \n",
       "1            -0.3  0.79       0  \n",
       "2             1.4  1.74       1  \n",
       "3            -0.8 -3.12       0  \n",
       "4            -0.3  0.79       0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the target variable to binary class (0 vs 1)\n",
    "students_df.Target = students_df.Target.replace({\"Graduate\": 0, \"Dropout\": 1})\n",
    "students_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5031063c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marital status                                      int64\n",
       "Application mode                                    int64\n",
       "Application order                                   int64\n",
       "Course                                              int64\n",
       "Daytime/evening attendance\\t                        int64\n",
       "Previous qualification                              int64\n",
       "Previous qualification (grade)                    float64\n",
       "Nacionality                                         int64\n",
       "Mother's qualification                              int64\n",
       "Father's qualification                              int64\n",
       "Mother's occupation                                 int64\n",
       "Father's occupation                                 int64\n",
       "Admission grade                                   float64\n",
       "Displaced                                           int64\n",
       "Educational special needs                           int64\n",
       "Debtor                                              int64\n",
       "Tuition fees up to date                             int64\n",
       "Gender                                              int64\n",
       "Scholarship holder                                  int64\n",
       "Age at enrollment                                   int64\n",
       "International                                       int64\n",
       "Curricular units 1st sem (credited)                 int64\n",
       "Curricular units 1st sem (enrolled)                 int64\n",
       "Curricular units 1st sem (evaluations)              int64\n",
       "Curricular units 1st sem (approved)                 int64\n",
       "Curricular units 1st sem (grade)                  float64\n",
       "Curricular units 1st sem (without evaluations)      int64\n",
       "Curricular units 2nd sem (credited)                 int64\n",
       "Curricular units 2nd sem (enrolled)                 int64\n",
       "Curricular units 2nd sem (evaluations)              int64\n",
       "Curricular units 2nd sem (approved)                 int64\n",
       "Curricular units 2nd sem (grade)                  float64\n",
       "Curricular units 2nd sem (without evaluations)      int64\n",
       "Unemployment rate                                 float64\n",
       "Inflation rate                                    float64\n",
       "GDP                                               float64\n",
       "Target                                              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d03bc",
   "metadata": {},
   "source": [
    "#### Train-Test split\n",
    "We will also need a simple method for shuffling and splitting the data into `train_set` and `test_set`. Latter is used for measuring the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c0b02a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2904, 36) (2904, 1)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(\n",
    "    df: pd.DataFrame, \n",
    "    test_size: float = 0.2) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Split the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The input features.\n",
    "        Y (np.ndarray): The target variable.\n",
    "        test_size (float): The size of the test set.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: The training and testing sets.\n",
    "    '''\n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # Calculate the number of rows in the test set\n",
    "    n_test_rows = int(len(df) * test_size)\n",
    "    # Split the dataset\n",
    "    train_df = df.iloc[:-n_test_rows]\n",
    "    test_df = df.iloc[-n_test_rows:]\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(students_df)\n",
    "X_train, Y_train = train_df.iloc[:, :-1].values, train_df.iloc[:, -1].values.reshape(-1,1)\n",
    "X_test, Y_test = test_df.iloc[:, :-1].values, test_df.iloc[:, -1].values.reshape(-1,1)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5267ca",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "And now... finally... let's train our model and see if it builds the tree (pick good splits) in a way that will be predicting future dropout students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0df8e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_0 is yrs_of_XP, X_1 is education_level\n",
      "------\n",
      "X_30 <= 3.0 ? Info Gain:  0.243\n",
      " left:X_22 <= 0.0 ? Info Gain:  0.053\n",
      "  left:X_16 <= 0.0 ? Info Gain:  0.051\n",
      "    left:X_33 <= 12.4 ? Info Gain:  0.040\n",
      "        left:1\n",
      "        right:1\n",
      "    right:X_12 <= 135.8 ? Info Gain:  0.049\n",
      "        left:1\n",
      "        right:0\n",
      "  right:X_24 <= 5.0 ? Info Gain:  0.011\n",
      "    left:X_30 <= 2.0 ? Info Gain:  0.003\n",
      "        left:1\n",
      "        right:1\n",
      "    right:X_30 <= 2.0 ? Info Gain:  0.114\n",
      "        left:1\n",
      "        right:0\n",
      " right:X_16 <= 0.0 ? Info Gain:  0.047\n",
      "  left:X_6 <= 143.0 ? Info Gain:  0.073\n",
      "    left:X_18 <= 0.0 ? Info Gain:  0.050\n",
      "        left:1\n",
      "        right:0\n",
      "    right:X_1 <= 39.0 ? Info Gain:  0.333\n",
      "        left:0\n",
      "        right:1\n",
      "  right:X_30 <= 4.0 ? Info Gain:  0.010\n",
      "    left:X_29 <= 9.0 ? Info Gain:  0.039\n",
      "        left:0\n",
      "        right:1\n",
      "    right:X_23 <= 11.0 ? Info Gain:  0.005\n",
      "        left:0\n",
      "        right:0\n"
     ]
    }
   ],
   "source": [
    "regression_tree = ClassificationTree(min_samples_split=3, max_depth=3)\n",
    "regression_tree.fit(X_train, Y_train)\n",
    "regression_tree.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9edbe",
   "metadata": {},
   "source": [
    "#### Measure accuracy\n",
    "\n",
    "The model predicts float values betwen $0$ and $1$. We will pick $0.5$ for a threshold and every value above this will be considered as $1$ (`Dropout`) and every value below will be considered as $0$ (`Graduate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c724cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 90%\n"
     ]
    }
   ],
   "source": [
    "Y_pred = regression_tree.predict(X_test) \n",
    "correct_predictions = 0\n",
    "for idx in range(len(X_test)):\n",
    "    prediction = Y_pred[idx]\n",
    "    # Convert to 0 and 1 (0 = Graduate, 1 = Dropout)\n",
    "    prediction = 1 if prediction > 0.5 else 0\n",
    "    actual = Y_test[idx][0]\n",
    "    if prediction == actual:\n",
    "        correct_predictions += 1\n",
    "\n",
    "print(f\"\\nAccuracy: {correct_predictions/len(X_test)*100:.0f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
